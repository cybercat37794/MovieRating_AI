# -*- coding: utf-8 -*-
"""movie_rating_model2_BERD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNO_rZPYPZ53sMwLd-BlHa0AHF1iaSac
"""

import pandas as pd
import numpy as np
from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras import mixed_precision
from sklearn.preprocessing import LabelEncoder

# Enable mixed precision training
policy = mixed_precision.Policy('mixed_float16')
# Changed set_policy to set_global_policy
mixed_precision.set_global_policy(policy)

# Load dataset
try:
    df = pd.read_csv('/content/clean_movie_datasetV2.csv')
except FileNotFoundError:
    print("Error: Dataset file not found. Please upload 'clean_movie_datasetV2.csv'.")
    exit()

# Preprocessing
df['plot'] = df['plot'].fillna('')
df['averageRating'] = df['averageRating'].astype(str)

# Encode the target labels
label_encoder = LabelEncoder()
df['averageRating'] = label_encoder.fit_transform(df['averageRating'])

# Tokenization
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')


def tokenize_plot(text):
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=256,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )
    return encoding['input_ids'], encoding['attention_mask']

# Create a TensorFlow dataset


def create_dataset(df):
    input_ids = []
    attention_masks = []
    for plot in df['plot']:
        ids, masks = tokenize_plot(plot)
        input_ids.append(ids)
        attention_masks.append(masks)
    input_ids = tf.concat(input_ids, axis=0)
    attention_masks = tf.concat(attention_masks, axis=0)
    labels = tf.convert_to_tensor(df['averageRating'].values, dtype=tf.int32)
    return tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids, 'attention_mask': attention_masks}, labels))


# Data Splitting
df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)

train_dataset = create_dataset(df_train).batch(
    32).prefetch(tf.data.experimental.AUTOTUNE)
test_dataset = create_dataset(df_test).batch(
    32).prefetch(tf.data.experimental.AUTOTUNE)

# Load DistilBERT model (with error handling)
try:
    distilbert_model = TFDistilBertModel.from_pretrained(
        'distilbert-base-uncased')
    print("DistilBERT model loaded successfully!")
except Exception as e:
    print(f"Error loading DistilBERT model: {e}")
    exit()

# Model Building
input_ids = tf.keras.layers.Input(
    shape=(256,), dtype=tf.int32, name='input_ids')
attention_mask = tf.keras.layers.Input(
    shape=(256,), dtype=tf.int32, name='attention_mask')

# Use DistilBERT model


def distilbert_layer(inputs):
    return distilbert_model(input_ids=inputs[0], attention_mask=inputs[1])[0]


distilbert_output = tf.keras.layers.Lambda(
    distilbert_layer, output_shape=(256, 768))([input_ids, attention_mask])

# Use the [CLS] token representation
pooled_output = distilbert_output[:, 0, :]

x = tf.keras.layers.Dense(256, activation='relu')(pooled_output)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.3)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(3, activation='softmax')(
    x)  # Use softmax for multi-class classification

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

# Optimizer and Compilation
optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=0.01)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Training with EarlyStopping
early_stopping = EarlyStopping(
    monitor='val_accuracy', patience=3, restore_best_weights=True)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)

batch_size = 32
epochs = 10

history = model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=epochs,
    callbacks=[early_stopping, reduce_lr]
)

# Evaluate the model
loss, accuracy = model.evaluate(test_dataset, verbose=0)
print(f"Accuracy on test set: {accuracy}")

# Predict and calculate additional metrics
y_pred_prob = model.predict(test_dataset)
y_pred = np.argmax(y_pred_prob, axis=1)

precision = precision_score(
    df_test['averageRating'], y_pred, average='weighted')
recall = recall_score(df_test['averageRating'], y_pred, average='weighted')
f1 = f1_score(df_test['averageRating'], y_pred, average='weighted')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
