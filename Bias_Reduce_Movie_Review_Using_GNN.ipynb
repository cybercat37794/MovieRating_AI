{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdITiWGtqLVH",
        "outputId": "4997bbfe-d1dc-48bb-c199-aa9575ded230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "edDVEOYhAKfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "texts = [\n",
        "    \"This film is absolutely fantastic!\",\n",
        "    \"I couldn't stand how boring this movie was.\",\n",
        "    \"The cast did a stellar job with their performances.\",\n",
        "    \"I found the plot to be incredibly predictable.\",\n",
        "    \"Such an amazing experience; I would recommend it to everyone!\",\n",
        "    \"The pacing was too slow for my taste.\",\n",
        "    \"I was completely captivated from start to finish.\",\n",
        "    \"This was a total waste of my time.\",\n",
        "    \"The cinematography was breathtakingly beautiful.\",\n",
        "    \"The storyline felt disjointed and confusing.\",\n",
        "    \"I really enjoyed the humor throughout the film.\",\n",
        "    \"The ending left me feeling unsatisfied.\",\n",
        "    \"A brilliant piece of art that deserves all the accolades!\",\n",
        "    \"The dialogue was cringe-worthy and unrealistic.\",\n",
        "    \"I couldn't help but smile during the heartwarming scenes.\",\n",
        "    \"This movie is overrated and lacks substance.\",\n",
        "    \"The character development was impressive and relatable.\",\n",
        "    \"I felt like I was watching paint dry.\",\n",
        "    \"The soundtrack perfectly complemented the emotional moments.\",\n",
        "    \"I found the special effects to be underwhelming.\",\n",
        "    \"The plot twists kept me on the edge of my seat!\",\n",
        "    \"I didn’t connect with any of the characters.\",\n",
        "    \"This is a film I’ll cherish for a long time.\",\n",
        "    \"The movie was filled with clichés and stereotypes.\",\n",
        "    \"I loved how it explored complex themes with depth.\",\n",
        "    \"It seemed to drag on forever without any real purpose.\",\n",
        "    \"The direction was innovative and fresh.\",\n",
        "    \"I felt completely lost and disengaged.\",\n",
        "    \"A masterpiece that challenges societal norms.\",\n",
        "    \"I was hoping for more from the climax.\",\n",
        "    \"The performances were so moving; I was in tears!\",\n",
        "    \"This movie is nothing but a cash grab.\",\n",
        "    \"I appreciated the subtle nuances in the writing.\",\n",
        "    \"The action sequences were poorly choreographed.\",\n",
        "    \"It made me reflect on important issues in our society.\",\n",
        "    \"The humor fell flat and felt forced.\",\n",
        "    \"A film that captures the essence of true friendship.\",\n",
        "    \"I couldn't help but roll my eyes at the plot.\",\n",
        "    \"The visuals were stunning and imaginative.\",\n",
        "    \"I found the characters to be one-dimensional.\",\n",
        "    \"This is a must-see for anyone who loves great storytelling.\",\n",
        "    \"The pacing was erratic and distracting.\",\n",
        "    \"It left me feeling uplifted and inspired.\",\n",
        "    \"The film lacked any real tension or excitement.\",\n",
        "    \"A delightful romp that is enjoyable for all ages!\",\n",
        "    \"I was disappointed with how they handled the climax.\",\n",
        "    \"The chemistry between the leads was electric!\",\n",
        "    \"It felt like a rehash of better films.\",\n",
        "    \"A heartfelt narrative that resonates with many.\",\n",
        "    \"The script was riddled with plot holes.\",\n",
        "    \"I enjoyed the clever twists throughout the story.\",\n",
        "    \"It had potential, but it missed the mark entirely.\",\n",
        "    \"A poignant exploration of love and loss.\",\n",
        "    \"The humor was juvenile and unoriginal.\",\n",
        "    \"I loved how it combined different genres seamlessly.\",\n",
        "    \"The ending was abrupt and confusing.\",\n",
        "    \"A thrilling ride that kept my heart racing!\",\n",
        "    \"The film felt like it was trying too hard to be clever.\",\n",
        "    \"A touching story that made me believe in love again.\",\n",
        "    \"The pacing was uneven and frustrating.\",\n",
        "    \"The film tackles relevant social issues beautifully.\",\n",
        "    \"It was a tedious watch that felt overly long.\",\n",
        "    \"I was charmed by the quirky characters and their arcs.\",\n",
        "    \"The narrative felt convoluted and chaotic.\",\n",
        "    \"A beautiful homage to classic cinema!\",\n",
        "    \"I was hoping for a deeper exploration of the themes.\",\n",
        "    \"The performances were heartfelt and genuine.\",\n",
        "    \"It felt like a missed opportunity to be groundbreaking.\",\n",
        "    \"The film is a breath of fresh air in today's cinema.\",\n",
        "    \"I didn't find the humor appealing at all.\",\n",
        "    \"A gripping tale that showcases the human spirit.\",\n",
        "    \"It was disappointing to see such a talented cast wasted.\",\n",
        "    \"The dialogues were witty and engaging.\",\n",
        "    \"It felt more like a commercial than a story.\",\n",
        "    \"A compelling journey that I won’t forget easily.\",\n",
        "    \"The conflicts felt contrived and forced.\",\n",
        "    \"This film beautifully captures the magic of childhood.\",\n",
        "    \"I struggled to stay engaged throughout the film.\",\n",
        "    \"A riveting narrative that keeps you guessing!\",\n",
        "    \"The visual effects were distracting and overdone.\",\n",
        "    \"A touching tribute that hits all the right notes.\",\n",
        "    \"I felt no emotional connection to the characters.\",\n",
        "    \"The twists were unexpected and refreshing.\",\n",
        "    \"It seemed to lack any real focus or direction.\",\n",
        "    \"A film that successfully blends drama and comedy.\",\n",
        "    \"The pacing was frustratingly slow.\",\n",
        "    \"A captivating exploration of self-discovery.\",\n",
        "    \"I was left wanting more depth and substance.\",\n",
        "    \"The film had its moments of brilliance.\",\n",
        "    \"It was a lackluster attempt at storytelling.\",\n",
        "    \"A wonderful film that deserves a place in your collection!\",\n",
        "    \"The character arcs were unconvincing and rushed.\",\n",
        "    \"A visually stunning film that is a feast for the eyes.\",\n",
        "    \"I found the humor to be out of place.\",\n",
        "    \"The film tackles complex issues with grace.\",\n",
        "    \"It felt like an endless parade of clichés.\",\n",
        "    \"A heartfelt story that will resonate with many.\",\n",
        "    \"I was unimpressed with the overall execution.\",\n",
        "    \"The chemistry between characters made it enjoyable.\",\n",
        "    \"A movie that makes you think and reflect deeply.\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,  # Negative\n",
        "    1,  # Positive\n",
        "    0,   # Negative\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0\n",
        "]\n",
        "# Define biased terms based on the text sentiments\n",
        "biased_terms = [\n",
        "0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,  # Biased\n",
        "    0,  # Unbiased\n",
        "    1,   # Biased\n",
        "    0,  # Biased\n",
        "    1,  # Unbiased\n",
        "    0,   # Biased\n",
        "    1,   # Biased\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "    0,\n",
        "    1,\n",
        "]\n",
        "\n",
        "assert len(texts) == len(labels), f\"Mismatch: {len(texts)} texts vs {len(labels)} labels\"\n",
        "\n",
        "\n",
        "# Create a simple graph representation\n",
        "edges = [(0, 1), (1, 2), (2, 3)]  # Example edges between nodes\n",
        "x = torch.eye(len(texts))  # Identity matrix as node features\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Create the graph data object\n",
        "data = Data(x=x, edge_index=edge_index, y=torch.tensor(labels), bias=torch.tensor(biased_terms))\n"
      ],
      "metadata": {
        "id": "HwxRGVHxqN9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 4)  # First GCN layer\n",
        "        self.conv2 = GCNConv(4, out_channels)  # Second GCN layer\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Z9XmcjijqWYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the training function\n",
        "def train(model, data, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        # Squeeze the output to shape [4] if needed\n",
        "        out = out.squeeze()  # This will change the shape from [4, 1] to [4]\n",
        "\n",
        "        # Standard loss\n",
        "        loss = nn.BCEWithLogitsLoss()(out, data.y.float())\n",
        "\n",
        "        # Debiasing loss\n",
        "        bias_loss = torch.mean(data.bias.float() * (1 - out.sigmoid()))  # Penalizing bias\n",
        "        total_loss = loss + 0.1 * bias_loss  # Combine losses\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {total_loss.item()}')\n"
      ],
      "metadata": {
        "id": "dvQC_A1tqbL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, optimizer, and training\n",
        "model = GCN(in_channels=len(texts), out_channels=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "train(model, data, optimizer, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFgZq04kqjPp",
        "outputId": "57a11035-0e68-49c9-d5fd-e12740fcd247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7227017879486084\n",
            "Epoch 10, Loss: 0.6868343949317932\n",
            "Epoch 20, Loss: 0.6312732100486755\n",
            "Epoch 30, Loss: 0.5434053540229797\n",
            "Epoch 40, Loss: 0.4239787757396698\n",
            "Epoch 50, Loss: 0.3011869490146637\n",
            "Epoch 60, Loss: 0.20946121215820312\n",
            "Epoch 70, Loss: 0.1542130410671234\n",
            "Epoch 80, Loss: 0.12354585528373718\n",
            "Epoch 90, Loss: 0.10637562721967697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        preds = out.sigmoid().round()  # Get binary predictions\n",
        "        accuracy = (preds == data.y.view_as(preds)).float().mean()\n",
        "        print(f'Accuracy: {accuracy.item()}')\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLEshOscqlhG",
        "outputId": "6933c240-92f3-497b-8ed6-360ef0da2528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9900000095367432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bias_reduction(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        preds = out.sigmoid().round()  # Get binary predictions\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    overall_accuracy = (preds == data.y.view_as(preds)).float().mean()\n",
        "    print(f'Overall Accuracy: {overall_accuracy.item()}')\n",
        "\n",
        "    # Evaluate bias metrics\n",
        "    group_0_indices = (data.bias == 0).nonzero(as_tuple=True)[0]\n",
        "    group_1_indices = (data.bias == 1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    group_0_accuracy = (preds[group_0_indices] == data.y[group_0_indices].view_as(preds[group_0_indices])).float().mean()\n",
        "    group_1_accuracy = (preds[group_1_indices] == data.y[group_1_indices].view_as(preds[group_1_indices])).float().mean()\n",
        "\n",
        "    print(f'Group 0 Accuracy: {group_0_accuracy.item()}')\n",
        "    print(f'Group 1 Accuracy: {group_1_accuracy.item()}')\n",
        "\n",
        "    # Calculate Disparate Impact\n",
        "    disparate_impact = group_0_accuracy / (group_1_accuracy + 1e-6)  # Avoid division by zero\n",
        "    print(f'Disparate Impact: {disparate_impact.item()}')\n",
        "\n",
        "# Evaluate the model for bias reduction\n",
        "evaluate_bias_reduction(model, data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb5PckUcrQ6N",
        "outputId": "85654e6c-ee73-41b3-96fe-1df694a1a0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.9900000095367432\n",
            "Group 0 Accuracy: 1.0\n",
            "Group 1 Accuracy: 0.9800000190734863\n",
            "Disparate Impact: 1.0204070806503296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgjg24VJs64s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
